						Hadoop学习笔记
1. Hadoop是一个适合海量数据的分布式存储和分布式计算的平台.
2. Hadoop包含三个组件:
	1> HDFS: 是一个分布式存储框架, 适合海量数据存储
		主从结构:
			NameNode: 接受用户操作请求, 是用户操作的入口.
					  维护文件系统目录结构, 称作命名空间.
					  是整个文件系统的管理节点, 维护着整个文件系统的文件目录树, 文件/目录的元信息和每个文件对应的
					  数据块列表.
			DataNode: 负责存储数据
					  文件块(block): 最基本的存储单位, 默认HDFS文件块的大小为128MB
	2> MapReduce: 是一个分布式计算框架, 适合海量数据的计算
		由两个阶段组成: Map 和 Reduce 
			Map 阶段是一个独立的程序, 有很多节点独立运行, 每个节点处理一部分数据.
				框架使用InputFormat类的子类把输入文件划分为很多个InputSplit, 默认, 每个HDFS的block对应一个InputSplit.
			通过RecordReader类, 把每个InputSplit解析成一个<k1, v1>, 其中这里的k1是每一行的偏移量, 
			默认情况下, 框架对InputSplit中的每一行解析成一个<k1, v1>. 一个InputSplit对应一个map task. 计算结果
			生成<k2, v2>.
			Reduce阶段也是一个独立的程序, 可以理解为一个单独的聚合程序.
				框架对map的输出结果<k2, v2>进行分区, 不同的分区中的<k2, v2> 由不同的reduce task处理, 默认只有一个分区.
			框架对每个分区中的数据, 按照k2进行排序, 分组(相同k2的v2分成一组), 分组不会减少<k2, v2>的数量
			shuffle过程将不同map task的相同分区的内容copy到相同的reduce节点上面, reduce的个数与map的分区个数相同
			
	3> YARN: 是一个资源调度平台, 负责给计算框架分配计算资源
		主从结构:
			ResourceManager: 集群资源的分配跟调度
							 只要实现ApplicationMaster接口才能被ResourceManager管理, 比如:MapReduce, Storm, Flink, Spark.
							启动和监控Application Master，监控NodeManager。
			NodeManager: 单节点资源的管理，处理ResourceManager的命令，处理来自APP Master的命令。
				NodeManager包含两个组件：
				Application Master: 任务调度，数据切分，为应用程序申请资源，并分配给内部任务、任务监控和容错。
				Container：对任务运行环境的抽象，封装了CPU，内存等多维资源以及环境变量、启动命令等任务运行相关信息
3. Hadoop伪分布式集群搭建
	1> 准备Linux环境: Java, 静态ip, hostname, hosts, iptables, chkconfig, ssh, 免密登录
		Java: export PATH=$PATH:$JAVA_HOME
		静态ip: 参考博客https://blog.csdn.net/aafeiyang/article/details/81533542
		hostname: hostnamectl set-hostname <hostname>
		iptables: 
			centos7: systemctl stop firewalld 停用防火墙, systemctl disable firewalld 开机禁用
			centos6: service iptables status, service iptables stop
		chkconfig: centos6中设置开机禁用防火墙, chkconfig iptables --list, chkconfig iptables off开机禁止启动防火墙
		ssh: ssh-keygen -t rsa生成秘钥, ssh-copy-id hostname将生成的秘钥添加的相应的主机上面
	2> 修改配置文件: $HADOOP_HOME/etc/hadoop
		hadoop-env.sh
			export JAVA_HOME=/data/soft/java/jdk_1.8.0_191
			export HADOOP_LOG_DIR=/data/hadoop_repo/logs/hadoop
		yarn-env.sh
			export JAVA_HOME=/data/soft/java/jdk_1.8.0_191
			export YARN_LOG_DIR=/data/hadoop_repo/logs/yarn
		mapred-env.sh 
			export JAVA_HOME=/data/soft/java/jdk_1.8.0_191
		core-site.xml
			<configuration>
				<property>
					<name>fs.defaultFS</name>
					<value>hdfs://learn:9000</value>
				</property>
				<property>
					<name>hadoop.tmp.dir</name>
					<value>/data/hadoop_repo</value>
			   </property>
			</configuration>
		hdfs-site.xml
			<configuration>
				<property>
					<name>dfs.replication</name>
					<value>1</value>   <!-- 伪分布式只有一个节点, 所以文件的备份数量为1 -->
				</property>
			</configuration>
		yarn-site.xml
			<configuration>
				<property>
					<name>yarn.nodemanager.aux-services</name>
					<value>mapreduce_shuffle</value>   <!-- 表示yarn上运行的是MapReduce程序 -->
				</property>
				<property>
					<name>yarn.resourcemanager.hostname</name>  <!-- 配置resourcemanager地址 -->
					<value>learn</value>
					<value></value>
				</property>
				<!-- 开启日志聚合 -->
				<property>
						<name>yarn.log-aggregation-enable</name>
						<value>true</value>
				</property>
				<property>
						<name>yarn.log.server.url</name>
						<value>http://learn:19888/jobhistory/logs/</value>
				</property>
				<!-- 配置日志过期时间,单位秒 -->
				<property>
						<name>yarn.log-aggregation.retain-seconds</name>
						<value>86400</value>
				</property>
				<!-- 当运行spark-shell master yarn时, 如果内存资源给的过小, yarn直接kill掉进程, 加下面的两个配置即可 -->
				<property>
						<name>yarn.nodemanager.vmem-check-enabled</name>
						<value>false</value>
						<description>Whether virtual memory limits will be enforced for containers</description>
				</property>
				<property>
						<name>yarn.nodemanager.vmem-pmem-ratio</name>
						<value>4</value>
						<description>Ratio between virtual memory to physical memory when setting memory limits for containers</description>
				</property>
			</configuration>
		mapred-site.xml    mv mapred-site.xml.template mapred-site.xml
			<configuration>
				<property>
					<name>mapreduce.framework.name</name>
					<value>yarn</value>
				</property>
				<!-- 配置历史服务器，查看日志 -->
				<property>
					<name>mapreduce.jobhistory.address</name>
					<value>http://learn:10020</value>
				</property>
				<property>
					<name>mapreduce.jobhistory.webapp.address</name>
					<value>http://learn:19888</value>
				</property>
			</configuration>
		slaves
			localhost
	3> 格式化hdfs
		bin/hdfs namenode -format   # 格式化操作不能重复执行, 要是需要重复执行加-force参数
	4> 启动集群
		全部启动集群所有的进程: sbin/start-all.sh, sbin/stop-all.sh
		单独启动hdfs(web端口为50070)和yarn(web端口8088): sbin/start-dfs.sh, sbin/stop-dfs.sh, sbin/start-yarn.sh
	5> jps命令或者通过访问web页面: http://learn:8088(yarn), http://learn:50070(hdfs)
	
4. HDFS shell 是我们操作分布式文件系统的一个客户端: bin/hdfs dfs -command URI, URI的格式: hdfs://learn:9000/dir/file
   大多数HDFS shell 和对应的Linux shell命令类似
	1> hdfs dfs -put -f localSrc hdfsSrc    将本地文件上传到hdfs上面, -f强制上传, 覆盖已经存在的文件
	2> hdfs dfs -get /hello.txt .         	将hdfs文件下载本地当前目录
	3> hdfs dfs -mkdir /data 				在hdfs上面创建文件夹
	4> hdfs dfs								显示命令, --help显示命令的使用方法
	
5. 停止yarn上面的application
	yarn application -kill <application_id>
	
6. 启动historyserver进程: 该进程的作用是把之前本来散落在nodemanager节点上面的日志统计收集到hdfs上面的指定目录中
	修改mapred-site.xml
	<!-- 配置历史服务器，查看日志 -->
	<property>
		<name>mapreduce.jobhistory.address</name>
		<value>http://learn:10020</value>
	</property>
	<property>
		<name>mapreduce.jobhistory.webapp.address</name>
		<value>http://learn:19888</value>
	</property>
	修改集群配置yarn-site.xml文件:
	<property> 
		<name>yarn.log-aggregation-enable</name>  
		<value>true</value>
	</property>
	<property>
		<name>yarn.log.server.url</name>
		<value>http://learn:19888/jobhistory/logs/</value>
	</property>
	重启集群--> 启动historyserver进程(在所有的nodemanager节点上面启动):
	执行: sbin/mr-jobhistory-daemon.sh start historyserver

7. 提交MapReduce任务到Hadoop集群:
   hadoop jar mapreduce-learn-1.0.0.jar com.xq.mapreduce.WordCount /inputpath /outputpath
   hadoop [command | classname] 输入路径 输出路径	
   
8. 配置文件说明：
	默认的配置文件存放位置：
	[core-default.xml]  --> 
		${HADOOP_HOME}/share/hadoop/common/hadoop-common-2.7.5.jar/core-default.xml
	[hdfs-default.xml]  --> 
		${HADOOP_HOME}/share/hadoop/hdfs/hadoop-hdfs-2.7.5.jar/hdfs-default.xml
	[yarn-default.xml]  --> 
		${HADOOP_HOME}/share/hadoop/yarn/hadoop-yarn-common-2.7.5.jar/yarn-default.xml
	[mapred-default.xml]-->
		${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.5.jar/mapred-default.xml
	自定义配置文件存放位置：${HADOOP_HOME}/etc/hadoop
	core-site.xml   
	hdfs-site.xml
	yarn-site.xml
	mapred-site.xml

9. 分布式集群搭建
	在伪分布式的基础上修改：
	hdfs-site.xml
		<property>
			<name>dfs.replication</name>
			<value>3</value>   <!-- 分布式datanode节点 -->
		</property>
		<property>
			<name>dfs.namenode.secondary.http-address</name>
			<value>http://lear1:50090</value>
		</property>
	slaves   datanode节点host
		learn1
		learn2
		learn3
	使用rsync命令，编写同步脚本，将配置文件同步到其他所有几点上面。
	在NameNode节点上面执行脚本：sh start-dfs.sh  启动所有的NameNode，SecondaryNameNode和DataNode
	在ResourceManager节点上面执行脚本：sh start-yarn.sh  启动所有的ResourceManager和NodeManager
	
10. 时间同步
	使用ntp同步时间，rpm -qa|grep ntp 查看是否安装了ntp 
	1>. 修改/etc/ntp.conf配置文件:
	restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap  放开注释
	新增：
	server 127.127.1.0
	fudge 127.127.127.1.0 stratum 10 
	2>. vi /etc/sysconfig/ntpd
	新增：SYNC_HWCLOCK=yes
	3>. service ntpd status 查看状态
	4>. service ntpd start  启动 
	5>. chkconfig ntpd on   配置开机启动
	6>. ntpdate learn       编写定时任务crontab，使用命令ntpdate learn, 表示从learn服务器同步时间

11. 克隆虚拟机需要修改的地方：
	1> /etc/udev/rules.d/70-persistent-ipoib.rules 删除文件中原来的数据，将eth1修改为eth0, 记录网卡地址，在设置静态ip的时候使用
	2> vi /etc/sysconfig/network-scripts/ifcfg-ens33 修改网卡地址：HWADDR为上面记录的网卡地址, 以及ip地址
	3> /etc/sysconfig/network     
		NETWORKING=yes    
		HOSTNAME=learn
	4> reboot重启机器
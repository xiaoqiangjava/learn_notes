					Spark学习笔记
1. Spark集群安装
	切记: 运行Spark任务时, Spark的包_后跟的是Scala的版本, 确保使用Spark的版本和Scala的版本保持一致
	官网下载spark安装包, 解压到指定目录
	修改conf下面的配置文件
	spark-env.sh
		JAVA_HOME=/data/soft/Java/jdk1.8.0_201   
		SCALA_HOME=/data/soft/scala/2.11.8   # 不指定也可以运行
		SPARK_MASTER_HOST=learn    # 当使用Zookeeper做HA时, 不需要指定master地址, 由Zookeeper选举
		SPARK_MASTER_PORT=7077
		HADOOP_CONF_DIR=/data/soft/hadoop-2.7.5/etc/hadoop    # 运行spark在yarn上时需要指定Hadoop配置文件地址
	slaves:
		learn   # 指定从节点主机信息

2. sbin/start-all.sh 启动spark集群, sbin/stop-all.sh 停止集群

3. bin/spark-submit --class org.apache.spark.examples.SparkPi \  # 指定程序运行的主类
					--master yarn     # spark://host:port, mesos://host:port, yarn, or local.
					--deploy-mode cluster   # client、cluster默认是client模式，可以直接在提交作业的时候查看日志，在提交端生成
											的JVM会一直等待所有的计算过程全部完成才退出，有两个功能：一个是提交，一个时监控jar运行；cluster模式在提交端生成的JVM在提交jar包后退出，他只有一个提交功能，然后在某一个Worker上面会生成一个Driver的JVM，该JVM监控jar的运行，等待所有的代码执行完毕才退出。
					--total-executor-cores 2
					--executor-memory 512m
					<app-jar>         # 指定程序运行的jar包
					[app options]	  # 指定程序运行时的参数, 可选
					
4. Spark Core：内核，是Spark中最重要的内容，相当于Hadoop中MapReduce
		Spark Core和MapReduce都是进行离线数据分析的
			SparkCore的核心：RDD（弹性分布式数据集），由分区组成

5. Spark SQL：相当于Hive、Pig
		支持Sql语句和DSL语句 ——》Spark任务（RDD）-》运行

6. Spark Streaming：相当于Storm（实时数据分析）
		本质：将连续的数据-》转换成不连续的数据DStream（离散流）：本质还是RDD
		
7. spark-shell
	本地模式:
		bin/spark-shell 
	集群模式:
		bin/spark-shell --master spark://learn:7077
	yarn模式:
		bin/spark-shell --master yarn
		启动报错: 当分配的内存较小时, yarn会直接kill掉进程, 修改配置文件即可
		vi yarn-site.xml
		<property>
                <name>yarn.nodemanager.vmem-check-enabled</name>
                <value>false</value>
                <description>Whether virtual memory limits will be enforced for containers</description>
        </property>
        <property>
                <name>yarn.nodemanager.vmem-pmem-ratio</name>
                <value>4</value>
                <description>Ratio between virtual memory to physical memory when setting memory limits for containers</description>
        </property>
	注意: 
		当以yarn的格式运行时, 读取文件的地址不能是本地文件, 需要指定hdfs上面的文件地址hdfs://learn:9000/xxx
		
8. 算子(RDD对象的函数或者方法)是分为两个类型的:
	> Transformation: 延时计算, 每执行一步, 都会生成一个新的RDD
		sc.textFile("hdfs://learn:9000") 如果指定的文件不存在, 不会报错, 因为是懒加载, 只有当Action执行时才会触发.
		flatMap 先map, 在flatten
		map
		reduceByKey
		sortBy
	> Action: 立即执行
		collect 
	RDD之间是存在依赖的: 宽依赖(父RDD与子RDD: 最多一对一), 窄依赖(父RDD与子RDD: 1对多)
	
9. 读取json文件生成DataFrame
	val video = spark.read
				.json("hdfs://learn:9000/videoCount/videoCount.txt")   # 读取json文件
				.select("uid", "duration")     # 查询需要的字段
				.na    	# na: DataFrameNaFunctions, 对DataFrame中值为Null或者NAN的值做处理
				.fill(0, Array("duration")) # 当字段duration的值为null或者NAN时, 使用0来填充
				.map(row => (row.getString(0), row.getLong(1)))  # 将每一行的字段转换为tuple
				.collect
				.toSeq
	DataFrameNaFunctions:
		处理DataFrame中值为Null或者NAN的字段
		na.fill(0)  # 使用指定的值填充所有值为Null或者NAN的列
		na.fill("A", Array("col_1", "col_2")).show()  # 当指定的列为null或者NAN时使用A来填充
		na.drop(1, Seq("col_1", "col_2"))  # 当指定列中不为null或者NAN的值少于一个时, 删除该记录
		na.drop().show()  # 只用某一行中有一列为null或者NAN, 则丢掉这条记录, 里面调用的是drop("any")
		na.drop("all").show() # 当某一行中所有的列都为null是丢掉该记录
		na.drop(Array("col_1", "col_2"))  # 当指定的列有一个为null时丢掉该记录
		na.drop("all", Array("col_1", "col_2")).show()   # 当指定的列都为null时丢掉该记录
		na.replace("col_1" :: "col_2" :: Nil, Map("UNKNOW" -> "Tomcat")) # 当col_1, col_2中的值为
		UNKNOW时, 用Tomcat替换
		
10. RDD, DataFrame, DataSet
	Rdd是SparkCore中的数据抽象, DataFrame和DataSet是SparkSQL的数据抽象. 与RDD相比, DataFrame多了数据
	的结构信息, 即schema, 知道每列的名称和类型.
	RDD是分布式的Java对象集合, DataFrame是Row对象的集合. 
	RDD属性：
	1> 一组分片（Partition），即数据集的基本组成单位。
		对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。
		用户可以在创建RDD时指定分片个数，如果没有指定，那么就会采用默认值。
		默认值就是程序所分配到的CPU Core的数目。
	2> 一个计算每个分区的函数
		Spar中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。
		compute函数会对迭代器进行复合，不需要保存每次计算的结果。
	3> RDD 之间的依赖关系
		RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。
		在部分分区数据丢失时，spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD所有分区数据进行重新计算。
	4> 一个Partitioner，即RDD分片函数
		当前spark中实现了两种类型的分片函数，一个时基于哈希的HashPartitioner
		另外一个时基于范围的RangePartitioner。
		只有对于key-value的RDD，才会有Partitioner, 非key-value的RDD，partitioner为None
		Partitioner函数不仅决定了RDD本身的分片数量，也决定了parent RDD shuffle时的分片数量
	5> 一个列表，存储存取每个Partitioner的优先位置（preferred location)
		对于一个HDFS文件来说，这个列表保存的就是每个Partiton所在的块的位置。
		按照“移动数据不如移动计算”的概念，Spark进行在进行任务调度的时候，会尽可能的将计算任务
		分配到其所要处理数据块的存储位置。
	DataFrame除了提供了比RDD更丰富的算子以外, 更重要的特点是提升执行效率, 减少数据读取以及执行计划的优化.
	DataFrame性能高主要体现在两个方面:
	> 定制化的内存管理: 数据以二进制的形式存在于非堆内存, 节省了大量的空间之外, 还摆脱了GC的限制. 
	> 优化的执行计划: 查询计划通过Spark catalyst optimiser进行优化
	DataFrame的缺点是编译器缺少安全类型检查.
	DataSet是DataFrame API的一个扩展, 是Spark最新的数据抽象. 用户友好的API分格, 既具有安全
	类型检查也具有DataFrame的查询优化特性.
	DataFrame = DataSet[Row], DataFrame是DataSet的一个特例, 泛型为Row

11. Spark SQL提供了两种方法, 将一个RDD转换为Dataset
	> 使用case class
	case class People(name: String, age: Int)
	sc.textFile("path").map(_.split(",")).map(fileds => People(fileds(0), fileds(1))).toDF
	> 使用StructType
	val schema = StructType(Seq(StructFiled("name", StringType, false), StructFiled("age", IntegerType, false)))
	更高级的写法: 在scala中Nil代表一个空集合, ::是往集合头部插入一个元素
	val schema = StructType(StructFiled("name", StringType, false) :: StructFiled("age", IntegerType, false) :: Nil)
		
12. Spark集群启动流程
	1>. 启动Master进程
	2>. Mster进程启动之后，会解析slaves配置文件，找到启动Worker的host，然后启动相应的的Worker，并发送注册信息给Worker
	3>. Worker开始与Master进行注册，把注册信息发送给Master
	4>. Master收到注册信息后，并保存到内存和磁盘里。Master给Worker发送注册成功的信息（masterUrl）
	5>. Worker收到Master的URL信息后，开始与Master建立心跳

13. Spark集群任务提交流程
	1>. Driver端的SparkSubmit进程和Master进程进行通信，创建一个非常重要的对象（SparkContext)
	2>. Master收到任务信息后，开始资源调度，和所有的Worker进行通信，找到较空闲的Worker，通知Worker启动Executor子进程
	3>. Executor进程启动之后开始与Driver通信，Driver开始把生成的任务提交到相应的Executor，Executor开始计算任务
		
14. checkpoint
	为什么要做checkpoint？
		运行出的中间结果往往很重要，所以为了保证数据的安全性，要checkpoint
		最好把数据checkpoint到HDFS，这样便于该集群所有几点访问到
		在checkpoint之前最好先cache一下，这样先把数据放到缓存
		便于运行任务的调用，也便于在checkpoint的时候直接从缓存拿数据
	在什么时候做checkpoint？
		在发生shuffle之后做checkpoint
	checkpoint步骤：
		1. 建立 checkpoint存储目录
			sc.setCheckpointDir("hdfs://learn:9000/ck)
		2. rdd.cache()
		3. rdd.checkpoint()
		
15. 使用spark连接jdbc时，本地运行可以不指定driver信息，但在集群上面运行时需要指定driver信息，否则报错	
		
		